---
title: "draft"
format: html
editor: visual
---

git config pull.rebase false

```{r, message=FALSE}
library(readr)
college <- read_csv("college.csv") 
auto <- read_csv("auto.csv")
counties <- read_csv("counties.csv")

library(dplyr)
```

## Choice of Topic

Our mission for this project was to create an r package structure that can be used to more directly handle normal forms of data. We want to incorporate functions that can aid in finding/building keys for different data sets, and test different normal forms. Our R package will be somewhat of a cousin to testdat, but will provide functions that more directly handle normal form information.

In this draft was have the following functions:

-   is.key() this function would allow any number of column names to be imputed and with a true/false results on whether or not the combination of columns is a key

-   find.keys() this would be able to reveal keys with the lowest number of column combinations that exist in the dataset, potentially an argument exists within this function of n= a number of columns, where the user can see the keys that exist from a certain number of columns and if none exist then the function would return an error

-   first.normal() results in true/false if the data set is in first normal form or not

-   second.normal() results in true/false if the data set is in second normal form or not

## Plan of Work Details

-   Draft outlines of each function, including information on:

    -   Expected input of the data

    -   Expected output of the data

    -   Plain English logic on how the function should operate

    -   Working draft code of each function

    -   Test function on different data sets

-   Future work

    -   Further explore stress testing function on the data

    -   Add optional arguments for a more customization data approach

    -   Analyze how missingness may impact functions

    -   Review Tidy Manifesto

### Example of is.key

is.key(data, column) or is.key(data\$column) or is.key(data, c(column1, column2, ...))

Logic: testing if there are duplicate observations within the listed column(s)

Output: TRUE or FALSE, optional argument to show duplicates in each column

Requirements: data = dataframe/tibble, no missing values?

Dependencies: Dplyr

#### Code for is.key

```{r}
is_key <- function(data, ..., optional_arguement = FALSE) {
  key_candidate <- data |> select(...)
  !any(duplicated(key_candidate))
}

is_key(college, c(stabbr,city))
is_key(college, unitid)

is_key(counties, c(state, county))
```

### Example of find.keys

find.keys(data)

Output: tibble with each observation being a comma separated list of the columns that make up a key (for maximum number of columns)

Output: if number of columns to make key = x, return error: no key of this size available

Dependency: purrr, dplyr, tibble

Logic: the function cycles through each column in the data, and first tests if there is any duplicates in the single columns. If the columns are distinct already, then that is a key. If not, the function moves on to combinations of columns and tests if they are distinct. The function will make combinations as large as the max.size argument, and lists all possible keys found.

#### Code for find.keys

```{r}
library(dplyr) 
library(purrr) 

find_keys <- function(data, max_size = 1) { 
  cols <- colnames(data) 
  n <- length(cols) 
  results <- list() 
  
  for (k in 1:min(max_size, n)) { 
    combos <- combn(cols, k, simplify = FALSE) 
    
    for (combo in combos) { 
      if (is_key(data, combo)) { 
        results <- append(results, list( 
          tibble(key_size = k, columns = paste(combo, collapse = ", ")) 
          )) 
        } 
      } 
    } 
  
  if (nrow(bind_rows(results)) == 0) { 
    return(paste0("No keys of size ", max_size, " found, pick larger max size")) 
    } else { 
      bind_rows(results) 
      } 
  } 

find_keys(college) 

find_keys(counties) 

find_keys(auto, max_size = 3)

```

### Example of first.normal

first.normal(data, max.size = n)

Output: TRUE/FALSE and reason if false

Logic: The first condition of first normal form is all atomic columns (tidy format basically) and then uses the find.key function to find a key. If the data is tidy and has a key, then the function will return true and the keys found, and if false it will return which condition is not met.

#### Code for first.normal

```{r}
first_normal <- function(data, max_size = 2) {
  
  is_atomic <- function(x) {
    all(!sapply(x, function(v) length(v) != 1 || is.list(v)))
  }
  
  atomic_cols <- sapply(data, is_atomic)
  
  if (!all(atomic_cols)) {
    return("FALSE: Dataset is NOT in First Normal Form, non-atomic columns found")
  }

      result <- find_keys(data, max_size = max_size)
      result
      
  if (is.character(result)) {
    return("FALSE: Dataset is NOT in First Normal Form, no candidate key exists")
  }
  
  return("TRUE")
}

first_normal(college)

first_normal(auto)
```

### Example of second.normal

second.normal(data, max.size = n)

Output: TRUE/FALSE and reason if false

Logic: The first condition of second normal form that it is in first normal form, so that is checked first. Next it needs to check for partial dependencies for keys with \>1 columns. For keys that may have partial dependencies, every possible subset combination will be created and tested for duplicates. If there are no duplicates, then the subset is a key are therefore has a partial dependency.

#### Code for second.normal

```{r}
second_normal <- function(data, max_size = 2) {

  result <- first_normal(data, max_size = max_size)

  if (!is.list(result)) {
    return("FALSE: Dataset is NOT in Second Normal Form because it is not in First Normal Form")
  }

  candidate_keys <- result$candidate_keys
  
  #data with single column keys cannot have partial dependencies
  if (any(candidate_keys$key_size == 1)) {
    return("TRUE: Dataset is in Second Normal Form due to single column key")
  }

  #checking for partial dependencies and loop through keys
  for (i in seq_len(nrow(candidate_keys))) {

    key_cols <- unlist(strsplit(candidate_keys$columns[i], ", "))

    if (length(key_cols) > 1) { 
      
      #create subsets of key
      subsets <- unlist(
        lapply(1:(length(key_cols) - 1),
               function(k) combn(key_cols, k, simplify = FALSE)
        ),
        recursive = FALSE
      )

      #check if any subset uniquely identifies rows
      for (subset in subsets) {

        if (!any(duplicated(data[subset]))) {
          return(paste0(
            "FALSE: Dataset is NOT in Second Normal Form. ",
            "Partial dependency detected: subset {",
            paste(subset, collapse = ", "),
            "} uniquely identifies rows."
          ))
        }
      }
    }
  }

  return(list(
    message = "TRUE",
    candidate_keys = candidate_keys
  ))
}

second_normal(auto)

second_normal(college)
```
